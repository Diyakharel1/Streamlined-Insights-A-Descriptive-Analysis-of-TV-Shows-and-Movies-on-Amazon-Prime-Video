# -*- coding: utf-8 -*-
"""AmazonPrimeVideo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RWjGRmtt5WbNnEs1Rz4-mMUSMeOpjxvC

# **Streamlined Insights: A Descriptive Analysis of TV Shows and Movies on Amazon Prime Video**

# Load and Understand the Data
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the CSV file
file_path = '/amazon_prime_video.csv'
amazon_prime_data = pd.read_csv(file_path)

# Display the first few rows of the dataframe
print(amazon_prime_data.head())
print("\n")

# Display data types and non-null counts
print(amazon_prime_data.info())
print("\n")

# Seeing the number of missing values in the dataset
missing_values = amazon_prime_data.isnull().sum().sum()
print("Total missing values in the dataset:", missing_values)
print("\n")

# Display basic statistics
print(amazon_prime_data.describe())

"""# Visualize Missing Values"""

# Check for missing values
missing_values_amazon = amazon_prime_data.isnull().sum()

# Visualize the missing values
plt.figure(figsize=(10, 6))
sns.barplot(x=missing_values_amazon.index, y=missing_values_amazon.values)
plt.title('Missing Values per Column before Cleaning')
plt.xticks(rotation=45)
plt.ylabel('Number of Missing Values')
plt.show()

"""# Filling missing values with mean or median

# Handling the 'duration' Column
"""

# Clean 'duration' column
amazon_prime_data['duration'] = amazon_prime_data['duration'].fillna('0 min')
amazon_prime_data['duration'] = amazon_prime_data['duration'].str.replace(' min', '').str.replace(' Season', 's').str.replace(' Seasons', 's')

# Separate 'duration' into 'duration_minutes' and 'duration_seasons'
amazon_prime_data['duration_minutes'] = amazon_prime_data['duration'].apply(lambda x: int(x) if 's' not in x else 0)
amazon_prime_data['duration_seasons'] = amazon_prime_data['duration'].apply(lambda x: int(x.replace('s', '')) if 's' in x else 0)

# Drop the original 'duration' column
amazon_prime_data.drop(columns=['duration'], inplace=True)

"""# Dealing with missing values"""

numerical_cols = ['duration_minutes', 'duration_seasons']

for col in numerical_cols:
    if col in amazon_prime_data.columns:
        amazon_prime_data[col].fillna(amazon_prime_data[col].median(), inplace=True)
    else:
        print(f"Column '{col}' not found in the dataset.")

# Check if there are any remaining missing values
print(amazon_prime_data.isnull().sum())

"""# Filling missing value with unknown"""

categorical_cols = ['director', 'cast', 'country', 'rating']
amazon_prime_data[categorical_cols] = amazon_prime_data[categorical_cols].fillna('Unknown')

# Check if there are any remaining missing values
print(amazon_prime_data.isnull().sum())

"""# Dropping values"""

# Drop rows with any missing values
amazon_prime_data.dropna(inplace=True)

# Check if there are any remaining missing values
print(amazon_prime_data.isnull().sum())

"""# visualizing after data cleaning"""

# Check for missing values
missing_values = amazon_prime_data.isnull().sum()

# Visualize the missing values
plt.figure(figsize=(10, 6))
sns.barplot(x=missing_values.index, y=missing_values.values)
plt.title('Missing Values per Column after Cleaning for Amazon Prime Video')
plt.xticks(rotation=45)
plt.ylabel('Number of Missing Values')
plt.show()

"""# Distribution of Categorical variables"""

# Distribution of content type
plt.figure(figsize=(8, 6))
sns.countplot(data=amazon_prime_data, x='type', palette='viridis')
plt.title('Distribution of Content Type on Amazon Prime Video')
plt.xlabel('Type')
plt.ylabel('Number of Content')
plt.show()

# Distribution of ratings
plt.figure(figsize=(12, 6))
sns.countplot(data=amazon_prime_data, x='rating', palette='coolwarm', order=amazon_prime_data['rating'].value_counts().index)
plt.title('Distribution of Ratings on Amazon Prime Video')
plt.xlabel('Rating')
plt.ylabel('Number of Content')
plt.xticks(rotation=45)
plt.show()

# Distribution of release year
plt.figure(figsize=(12, 6))
sns.histplot(amazon_prime_data['release_year'], bins=30, kde=True, color='purple')
plt.title('Distribution of Release Year on Amazon Prime Video')
plt.xlabel('Release Year')
plt.ylabel('Count')
plt.show()

"""# Distribution of Numerical Variables"""

# Distribution of release year for Amazon Prime Video TV shows and movies
plt.figure(figsize=(12, 6))
sns.histplot(amazon_prime_data['release_year'], bins=30, kde=True, color='green')
plt.title('Distribution of Release Year')
plt.xlabel('Release Year')
plt.ylabel('Count')
plt.show()

"""# Descriptive statistics for Numercial Variables"""

# Summary statistics for numerical variables
print(amazon_prime_data[['release_year', 'duration_minutes', 'duration_seasons']].describe())

# Additional statistics for numerical variables
def additional_stats(df):
    stats = df.agg(['mean', 'median', 'std', 'var', 'skew', 'kurt']).T
    stats['iqr'] = df.quantile(0.75) - df.quantile(0.25)
    return stats

print(additional_stats(amazon_prime_data[['release_year', 'duration_minutes', 'duration_seasons']]))

# Frequency tables for categorical variables
categorical_cols = ['type', 'rating', 'director', 'cast', 'country']

for col in categorical_cols:
    print(f"Frequency table for {col}:")
    print(amazon_prime_data[col].value_counts())
    print()

"""# Frequency Tables for Categorical Variables

# Box Plots for Outlier Detection
"""

# Box plot for duration in minutes
plt.figure(figsize=(12, 6))
sns.boxplot(data=amazon_prime_data, x='type', y='duration_minutes')
plt.title('Box Plot of Duration Minutes by Type')
plt.xlabel('Type')
plt.ylabel('Duration Minutes')
plt.show()

"""# Clustering"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Select numerical columns for clustering
numeric_cols = ['release_year', 'duration_minutes', 'duration_seasons']

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(amazon_prime_data[numeric_cols])

# Apply K-means clustering
kmeans = KMeans(n_clusters=3, random_state=42)
amazon_prime_data['cluster'] = kmeans.fit_predict(scaled_data)

# Calculate silhouette score
silhouette_avg = silhouette_score(scaled_data, amazon_prime_data['cluster'])
print(f"Silhouette Score: {silhouette_avg}")

# Visualize the clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(data=amazon_prime_data, x='release_year', y='duration_minutes', hue='cluster', palette='viridis')
plt.title('Clusters based on Release Year and Duration')
plt.xlabel('Release Year')
plt.ylabel('Duration Minutes')
plt.show()

# Cluster centers
cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)
cluster_centers_df = pd.DataFrame(cluster_centers, columns=numeric_cols)
cluster_centers_df['cluster'] = range(1, len(cluster_centers) + 1)
print("Cluster Centers:")
print(cluster_centers_df)

"""# Association Rule Mining"""

from mlxtend.frequent_patterns import apriori, association_rules

# Prepare the data for association rule mining
# For simplicity, we'll create a binary matrix for selected columns
binary_matrix = pd.get_dummies(amazon_prime_data[['type', 'rating']])

# Apply the Apriori algorithm
frequent_itemsets = apriori(binary_matrix, min_support=0.1, use_colnames=True)

# Generate the association rules
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.5)

# Display the rules
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

"""# Analysing the Description column with word cloud"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Combine all descriptions into a single string
text = ' '.join(amazon_prime_data['description'].dropna().tolist())

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

# Display the word cloud using matplotlib
plt.figure(figsize=(15, 9))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Netflix Descriptions')
plt.show()